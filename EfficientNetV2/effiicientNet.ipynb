{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing folder: B0001 ===\n",
      "  -> Annotation file: Baggages\\B0001\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (14, 5)\n",
      "  -> Detected 2D annotation array with 14 rows.\n",
      "  -> Processed 14 annotations across 14 images.\n",
      "  -> Labeling results: Pos=14, Neg=0, Total=14\n",
      "\n",
      "=== Processing folder: B0002 ===\n",
      "  -> Annotation file: Baggages\\B0002\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (8, 5)\n",
      "  -> Detected 2D annotation array with 8 rows.\n",
      "  -> Processed 8 annotations across 8 images.\n",
      "  -> Labeling results: Pos=8, Neg=1, Total=9\n",
      "\n",
      "=== Processing folder: B0003 ===\n",
      "  -> Annotation file: Baggages\\B0003\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (9, 5)\n",
      "  -> Detected 2D annotation array with 9 rows.\n",
      "  -> Processed 9 annotations across 9 images.\n",
      "  -> Labeling results: Pos=9, Neg=1, Total=10\n",
      "\n",
      "=== Processing folder: B0004 ===\n",
      "  -> Annotation file: Baggages\\B0004\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (9, 5)\n",
      "  -> Detected 2D annotation array with 9 rows.\n",
      "  -> Processed 9 annotations across 9 images.\n",
      "  -> Labeling results: Pos=9, Neg=0, Total=9\n",
      "\n",
      "=== Processing folder: B0005 ===\n",
      "  -> Annotation file: Baggages\\B0005\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (20, 5)\n",
      "  -> Detected 2D annotation array with 20 rows.\n",
      "  -> Processed 20 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=0, Total=10\n",
      "\n",
      "=== Processing folder: B0006 ===\n",
      "  -> Annotation file: Baggages\\B0006\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (10, 5)\n",
      "  -> Detected 2D annotation array with 10 rows.\n",
      "  -> Processed 10 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=0, Total=10\n",
      "\n",
      "=== Processing folder: B0007 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=20, Neg=0, Total=20\n",
      "\n",
      "=== Processing folder: B0008 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=361, Neg=0, Total=361\n",
      "\n",
      "=== Processing folder: B0009 ===\n",
      "  -> Annotation file: Baggages\\B0009\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0010 ===\n",
      "  -> Annotation file: Baggages\\B0010\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (11, 5)\n",
      "  -> Detected 2D annotation array with 11 rows.\n",
      "  -> Processed 11 annotations across 11 images.\n",
      "  -> Labeling results: Pos=11, Neg=0, Total=11\n",
      "\n",
      "=== Processing folder: B0011 ===\n",
      "  -> Annotation file: Baggages\\B0011\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (10, 5)\n",
      "  -> Detected 2D annotation array with 10 rows.\n",
      "  -> Processed 10 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=0, Total=10\n",
      "\n",
      "=== Processing folder: B0012 ===\n",
      "  -> Annotation file: Baggages\\B0012\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0013 ===\n",
      "  -> Annotation file: Baggages\\B0013\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (10, 5)\n",
      "  -> Detected 2D annotation array with 10 rows.\n",
      "  -> Processed 10 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=0, Total=10\n",
      "\n",
      "=== Processing folder: B0014 ===\n",
      "  -> Annotation file: Baggages\\B0014\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0015 ===\n",
      "  -> Annotation file: Baggages\\B0015\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0016 ===\n",
      "  -> Annotation file: Baggages\\B0016\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0017 ===\n",
      "  -> Annotation file: Baggages\\B0017\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0018 ===\n",
      "  -> Annotation file: Baggages\\B0018\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0019 ===\n",
      "  -> Annotation file: Baggages\\B0019\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (6, 5)\n",
      "  -> Detected 2D annotation array with 6 rows.\n",
      "  -> Processed 6 annotations across 6 images.\n",
      "  -> Labeling results: Pos=6, Neg=0, Total=6\n",
      "\n",
      "=== Processing folder: B0020 ===\n",
      "  -> Annotation file: Baggages\\B0020\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0021 ===\n",
      "  -> Annotation file: Baggages\\B0021\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0022 ===\n",
      "  -> Annotation file: Baggages\\B0022\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (6, 5)\n",
      "  -> Detected 2D annotation array with 6 rows.\n",
      "  -> Processed 6 annotations across 6 images.\n",
      "  -> Labeling results: Pos=6, Neg=0, Total=6\n",
      "\n",
      "=== Processing folder: B0023 ===\n",
      "  -> Annotation file: Baggages\\B0023\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (6, 5)\n",
      "  -> Detected 2D annotation array with 6 rows.\n",
      "  -> Processed 6 annotations across 6 images.\n",
      "  -> Labeling results: Pos=6, Neg=0, Total=6\n",
      "\n",
      "=== Processing folder: B0024 ===\n",
      "  -> Annotation file: Baggages\\B0024\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0025 ===\n",
      "  -> Annotation file: Baggages\\B0025\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0026 ===\n",
      "  -> Annotation file: Baggages\\B0026\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0027 ===\n",
      "  -> Annotation file: Baggages\\B0027\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0028 ===\n",
      "  -> Annotation file: Baggages\\B0028\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0029 ===\n",
      "  -> Annotation file: Baggages\\B0029\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (7, 5)\n",
      "  -> Detected 2D annotation array with 7 rows.\n",
      "  -> Processed 7 annotations across 7 images.\n",
      "  -> Labeling results: Pos=7, Neg=0, Total=7\n",
      "\n",
      "=== Processing folder: B0030 ===\n",
      "  -> Annotation file: Baggages\\B0030\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (7, 5)\n",
      "  -> Detected 2D annotation array with 7 rows.\n",
      "  -> Processed 7 annotations across 7 images.\n",
      "  -> Labeling results: Pos=7, Neg=0, Total=7\n",
      "\n",
      "=== Processing folder: B0031 ===\n",
      "  -> Annotation file: Baggages\\B0031\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0032 ===\n",
      "  -> Annotation file: Baggages\\B0032\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0033 ===\n",
      "  -> Annotation file: Baggages\\B0033\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (5, 5)\n",
      "  -> Detected 2D annotation array with 5 rows.\n",
      "  -> Processed 5 annotations across 5 images.\n",
      "  -> Labeling results: Pos=5, Neg=0, Total=5\n",
      "\n",
      "=== Processing folder: B0034 ===\n",
      "  -> Annotation file: Baggages\\B0034\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (6, 5)\n",
      "  -> Detected 2D annotation array with 6 rows.\n",
      "  -> Processed 6 annotations across 6 images.\n",
      "  -> Labeling results: Pos=6, Neg=0, Total=6\n",
      "\n",
      "=== Processing folder: B0035 ===\n",
      "  -> Annotation file: Baggages\\B0035\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 4 images.\n",
      "  -> Labeling results: Pos=4, Neg=0, Total=4\n",
      "\n",
      "=== Processing folder: B0036 ===\n",
      "  -> Annotation file: Baggages\\B0036\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (11, 5)\n",
      "  -> Detected 2D annotation array with 11 rows.\n",
      "  -> Processed 11 annotations across 11 images.\n",
      "  -> Labeling results: Pos=11, Neg=0, Total=11\n",
      "\n",
      "=== Processing folder: B0037 ===\n",
      "  -> Annotation file: Baggages\\B0037\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (11, 5)\n",
      "  -> Detected 2D annotation array with 11 rows.\n",
      "  -> Processed 11 annotations across 11 images.\n",
      "  -> Labeling results: Pos=11, Neg=0, Total=11\n",
      "\n",
      "=== Processing folder: B0038 ===\n",
      "  -> Annotation file: Baggages\\B0038\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (10, 5)\n",
      "  -> Detected 2D annotation array with 10 rows.\n",
      "  -> Processed 10 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=1, Total=11\n",
      "\n",
      "=== Processing folder: B0039 ===\n",
      "  -> Annotation file: Baggages\\B0039\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (9, 5)\n",
      "  -> Detected 2D annotation array with 9 rows.\n",
      "  -> Processed 9 annotations across 9 images.\n",
      "  -> Labeling results: Pos=9, Neg=0, Total=9\n",
      "\n",
      "=== Processing folder: B0040 ===\n",
      "  -> Annotation file: Baggages\\B0040\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (12, 5)\n",
      "  -> Detected 2D annotation array with 12 rows.\n",
      "  -> Processed 12 annotations across 12 images.\n",
      "  -> Labeling results: Pos=12, Neg=0, Total=12\n",
      "\n",
      "=== Processing folder: B0041 ===\n",
      "  -> Annotation file: Baggages\\B0041\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (10, 5)\n",
      "  -> Detected 2D annotation array with 10 rows.\n",
      "  -> Processed 10 annotations across 10 images.\n",
      "  -> Labeling results: Pos=10, Neg=0, Total=10\n",
      "\n",
      "=== Processing folder: B0042 ===\n",
      "  -> Annotation file: Baggages\\B0042\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (19, 5)\n",
      "  -> Detected 2D annotation array with 19 rows.\n",
      "  -> Processed 19 annotations across 19 images.\n",
      "  -> Labeling results: Pos=19, Neg=0, Total=19\n",
      "\n",
      "=== Processing folder: B0043 ===\n",
      "  -> Annotation file: Baggages\\B0043\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (9, 5)\n",
      "  -> Detected 2D annotation array with 9 rows.\n",
      "  -> Processed 9 annotations across 9 images.\n",
      "  -> Labeling results: Pos=9, Neg=0, Total=9\n",
      "\n",
      "=== Processing folder: B0044 ===\n",
      "  -> Annotation file: Baggages\\B0044\\Pmatrices.mat (Type: pmatrices)\n",
      "  Annotation array shape: (3, 4, 178)\n",
      "  -> Detected 3D annotation array with 178 instances.\n",
      "  -> Processed 178 annotations across 178 images.\n",
      "  -> Labeling results: Pos=1, Neg=177, Total=178\n",
      "\n",
      "=== Processing folder: B0045 ===\n",
      "  -> Annotation file: Baggages\\B0045\\ground_truth.txt (Type: ground_truth)\n",
      "  Annotation array shape: (80, 5)\n",
      "  -> Detected 2D annotation array with 80 rows.\n",
      "  -> Processed 80 annotations across 80 images.\n",
      "  -> Labeling results: Pos=80, Neg=10, Total=90\n",
      "\n",
      "=== Processing folder: B0046 ===\n",
      "  -> Annotation file: Baggages\\B0046\\ground_truth.txt (Type: ground_truth)\n",
      "  Annotation array shape: (209, 9)\n",
      "  -> Detected 2D annotation array with 209 rows.\n",
      "  -> Processed 209 annotations across 179 images.\n",
      "  -> Labeling results: Pos=179, Neg=21, Total=200\n",
      "\n",
      "=== Processing folder: B0047 ===\n",
      "  -> Annotation file: Baggages\\B0047\\ground_truth.txt (Type: ground_truth)\n",
      "  Annotation array shape: (209, 9)\n",
      "  -> Detected 2D annotation array with 209 rows.\n",
      "  -> Processed 209 annotations across 179 images.\n",
      "  -> Labeling results: Pos=179, Neg=21, Total=200\n",
      "\n",
      "=== Processing folder: B0048 ===\n",
      "  -> Annotation file: Baggages\\B0048\\ground_truth.txt (Type: ground_truth)\n",
      "  Annotation array shape: (209, 9)\n",
      "  -> Detected 2D annotation array with 209 rows.\n",
      "  -> Processed 209 annotations across 179 images.\n",
      "  -> Labeling results: Pos=179, Neg=21, Total=200\n",
      "\n",
      "=== Processing folder: B0049 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=200, Neg=0, Total=200\n",
      "\n",
      "=== Processing folder: B0050 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=100, Neg=0, Total=100\n",
      "\n",
      "=== Processing folder: B0051 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=100, Neg=0, Total=100\n",
      "\n",
      "=== Processing folder: B0052 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=144, Neg=0, Total=144\n",
      "\n",
      "=== Processing folder: B0053 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=144, Neg=0, Total=144\n",
      "\n",
      "=== Processing folder: B0054 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=144, Neg=0, Total=144\n",
      "\n",
      "=== Processing folder: B0055 ===\n",
      "  -> Annotation file: Baggages\\B0055\\labels.txt (Type: labels)\n",
      "  Annotation array shape: (199,)\n",
      "  -> Unsupported array dimension: 1\n",
      "  -> Processed 0 annotations across 0 images.\n",
      "  -> Labeling results: Pos=800, Neg=0, Total=800\n",
      "\n",
      "=== Processing folder: B0056 ===\n",
      "  -> Annotation file: Baggages\\B0056\\labels.txt (Type: labels)\n",
      "  Annotation array shape: (199,)\n",
      "  -> Unsupported array dimension: 1\n",
      "  -> Processed 0 annotations across 0 images.\n",
      "  -> Labeling results: Pos=1200, Neg=0, Total=1200\n",
      "\n",
      "=== Processing folder: B0057 ===\n",
      "  -> Annotation file: Baggages\\B0057\\labels.txt (Type: labels)\n",
      "  Annotation array shape: (199,)\n",
      "  -> Unsupported array dimension: 1\n",
      "  -> Processed 0 annotations across 0 images.\n",
      "  -> Labeling results: Pos=1600, Neg=0, Total=1600\n",
      "\n",
      "=== Processing folder: B0058 ===\n",
      "  -> Annotation file: Baggages\\B0058\\labels.txt (Type: labels)\n",
      "  Annotation array shape: (63,)\n",
      "  -> Unsupported array dimension: 1\n",
      "  -> Processed 0 annotations across 0 images.\n",
      "  -> Labeling results: Pos=64, Neg=0, Total=64\n",
      "\n",
      "=== Processing folder: B0059 ===\n",
      "  -> Annotation file: Baggages\\B0059\\labels.txt (Type: labels)\n",
      "  Annotation array shape: (63,)\n",
      "  -> Unsupported array dimension: 1\n",
      "  -> Processed 0 annotations across 0 images.\n",
      "  -> Labeling results: Pos=64, Neg=0, Total=64\n",
      "\n",
      "=== Processing folder: B0060 ===\n",
      "  -> Annotation file: Baggages\\B0060\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (4, 5)\n",
      "  -> Detected 2D annotation array with 4 rows.\n",
      "  -> Processed 4 annotations across 2 images.\n",
      "  -> Labeling results: Pos=2, Neg=0, Total=2\n",
      "\n",
      "=== Processing folder: B0061 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=21, Neg=0, Total=21\n",
      "\n",
      "=== Processing folder: B0062 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=22, Neg=0, Total=22\n",
      "\n",
      "=== Processing folder: B0063 ===\n",
      "  -> Annotation file: Baggages\\B0063\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (18, 5)\n",
      "  -> Detected 2D annotation array with 18 rows.\n",
      "  -> Processed 18 annotations across 18 images.\n",
      "  -> Labeling results: Pos=18, Neg=1, Total=19\n",
      "\n",
      "=== Processing folder: B0064 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=19, Neg=0, Total=19\n",
      "\n",
      "=== Processing folder: B0065 ===\n",
      "  -> Annotation file: Baggages\\B0065\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (20, 5)\n",
      "  -> Detected 2D annotation array with 20 rows.\n",
      "  -> Processed 20 annotations across 20 images.\n",
      "  -> Labeling results: Pos=20, Neg=1, Total=21\n",
      "\n",
      "=== Processing folder: B0066 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=22, Neg=0, Total=22\n",
      "\n",
      "=== Processing folder: B0067 ===\n",
      "  -> Annotation file: Baggages\\B0067\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (13, 5)\n",
      "  -> Detected 2D annotation array with 13 rows.\n",
      "  -> Processed 13 annotations across 13 images.\n",
      "  -> Labeling results: Pos=13, Neg=4, Total=17\n",
      "\n",
      "=== Processing folder: B0068 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=20, Neg=0, Total=20\n",
      "\n",
      "=== Processing folder: B0069 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=25, Neg=0, Total=25\n",
      "\n",
      "=== Processing folder: B0070 ===\n",
      "  -> Annotation file: Baggages\\B0070\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (21, 5)\n",
      "  -> Detected 2D annotation array with 21 rows.\n",
      "  -> Processed 21 annotations across 21 images.\n",
      "  -> Labeling results: Pos=21, Neg=0, Total=21\n",
      "\n",
      "=== Processing folder: B0071 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=22, Neg=0, Total=22\n",
      "\n",
      "=== Processing folder: B0072 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=22, Neg=0, Total=22\n",
      "\n",
      "=== Processing folder: B0073 ===\n",
      "  -> Annotation file: Baggages\\B0073\\BoundingBox.mat (Type: mat)\n",
      "  Annotation array shape: (20, 5)\n",
      "  -> Detected 2D annotation array with 20 rows.\n",
      "  -> Processed 20 annotations across 20 images.\n",
      "  -> Labeling results: Pos=20, Neg=0, Total=20\n",
      "\n",
      "=== Processing folder: B0074 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=37, Neg=0, Total=37\n",
      "\n",
      "=== Processing folder: B0075 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=576, Neg=0, Total=576\n",
      "\n",
      "=== Processing folder: B0076 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=576, Neg=0, Total=576\n",
      "\n",
      "=== Processing folder: B0077 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=576, Neg=0, Total=576\n",
      "\n",
      "=== Processing folder: B0078 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=0, Neg=490, Total=490\n",
      "\n",
      "=== Processing folder: B0079 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=150, Neg=0, Total=150\n",
      "\n",
      "=== Processing folder: B0080 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=150, Neg=0, Total=150\n",
      "\n",
      "=== Processing folder: B0081 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=150, Neg=0, Total=150\n",
      "\n",
      "=== Processing folder: B0082 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=0, Neg=581, Total=581\n",
      "\n",
      "=== Processing folder: B0083 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=0, Neg=48, Total=48\n",
      "\n",
      "=== Processing folder: B0084 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=50, Neg=0, Total=50\n",
      "\n",
      "=== Processing folder: B0085 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=0, Neg=48, Total=48\n",
      "\n",
      "=== Processing folder: B0086 ===\n",
      "  -> Annotation file: None (Type: None)\n",
      "  -> No annotation file found.\n",
      "  -> Labeling results: Pos=0, Neg=1000, Total=1000\n",
      "\n",
      "=== Final Summary ===\n",
      "Total images: 10817 (Positive: 8391, Negative: 2426)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Set your dataset root folder path\n",
    "dataset_root = 'Baggages'\n",
    "data_entries = []\n",
    "# Folders that should not be treated as positive despite having a 'labels' file\n",
    "EXCLUDED_FOLDERS = {'B0078', 'B0082', 'B0083', 'B0085', 'B0086'}\n",
    "\n",
    "def load_annotations_from_mat(mat_file, ann_type):\n",
    "    \"\"\"\n",
    "    Loads annotation data from a .mat file, considering the annotation type.\n",
    "    For 'pmatrices' type, prioritizes keys containing 'pmatrices' in their name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mat_data = loadmat(mat_file)\n",
    "        keys = [key for key in mat_data.keys() if not key.startswith('__')]\n",
    "        if ann_type == 'pmatrices':\n",
    "            # Look for keys likely to contain pmatrices data\n",
    "            pmatrix_keys = [k for k in keys if 'pmatrices' in k.lower()]\n",
    "            if pmatrix_keys:\n",
    "                selected_key = pmatrix_keys[0]\n",
    "            elif keys:\n",
    "                selected_key = keys[0]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            selected_key = keys[0] if keys else None\n",
    "        return mat_data[selected_key] if selected_key else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading MAT file {mat_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_annotations_from_text(text_file):\n",
    "    \"\"\"\n",
    "    Loads annotation data from a text file (e.g., labels or ground_truth).\n",
    "    Uses latin1 encoding and skips the first row (assuming it's a header).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_array = np.loadtxt(text_file, encoding='latin1', skiprows=1)\n",
    "        return data_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading text annotation file {text_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_annotation_file(folder_path):\n",
    "    \"\"\"\n",
    "    Search for an annotation file in the folder with priority:\n",
    "      1. \"labels\" text file\n",
    "      2. .mat files (prioritizing 'pmatrices' in name)\n",
    "      3. \"groundtruth\" or \"ground_truth\" files\n",
    "    \"\"\"\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().startswith(\"labels\"):\n",
    "            return os.path.join(folder_path, file), \"labels\"\n",
    "\n",
    "    mat_files = glob.glob(os.path.join(folder_path, \"*.mat\"))\n",
    "    if mat_files:\n",
    "        for f in mat_files:\n",
    "            if \"pmatrices\" in os.path.basename(f).lower():\n",
    "                return f, \"pmatrices\"\n",
    "        return mat_files[0], \"mat\"\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if \"groundtruth\" in file.lower() or \"ground_truth\" in file.lower():\n",
    "            return os.path.join(folder_path, file), \"ground_truth\"\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def process_annotation_array(data_array, ann_type, folder_path):\n",
    "    \"\"\"\n",
    "    Processes annotation array into a dictionary mapping image IDs to bounding boxes.\n",
    "    Handles both 2D and 3D arrays.\n",
    "    \"\"\"\n",
    "    gt_dict = {}\n",
    "    pos_rows = 0\n",
    "    if data_array.ndim == 3:\n",
    "        num_ann = data_array.shape[2]\n",
    "        print(f\"  -> Detected 3D annotation array with {num_ann} instances.\")\n",
    "        for i in range(num_ann):\n",
    "            ann_instance = data_array[:, :, i].flatten()\n",
    "            try:\n",
    "                image_id = int(ann_instance[0])\n",
    "            except Exception as e:\n",
    "                print(f\"    Skipping instance {i} in {folder_path}: {e}\")\n",
    "                continue\n",
    "            bbox = ann_instance[1:].tolist()\n",
    "            gt_dict.setdefault(image_id, []).append(bbox)\n",
    "            pos_rows += 1\n",
    "    elif data_array.ndim == 2:\n",
    "        num_ann = data_array.shape[0]\n",
    "        print(f\"  -> Detected 2D annotation array with {num_ann} rows.\")\n",
    "        for i, row in enumerate(data_array):\n",
    "            try:\n",
    "                image_id = int(np.squeeze(row[0]))\n",
    "            except Exception as e:\n",
    "                print(f\"    Skipping row {i} in {folder_path}: {e}\")\n",
    "                continue\n",
    "            bbox = row[1:].tolist()\n",
    "            gt_dict.setdefault(image_id, []).append(bbox)\n",
    "            pos_rows += 1\n",
    "    else:\n",
    "        print(f\"  -> Unsupported array dimension: {data_array.ndim}\")\n",
    "    print(f\"  -> Processed {pos_rows} annotations across {len(gt_dict)} images.\")\n",
    "    return gt_dict\n",
    "\n",
    "# Main processing loop\n",
    "for folder in sorted(os.listdir(dataset_root)):\n",
    "    folder_path = os.path.join(dataset_root, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing folder: {folder} ===\")\n",
    "    ann_file, ann_type = find_annotation_file(folder_path)\n",
    "    print(f\"  -> Annotation file: {ann_file} (Type: {ann_type})\")\n",
    "\n",
    "    # Determine if this folder should be treated as all-positive\n",
    "    is_labels_folder = (ann_type == 'labels') and (folder not in EXCLUDED_FOLDERS)\n",
    "    gt_dict = None\n",
    "    data_array = None\n",
    "\n",
    "    if ann_file:\n",
    "        if ann_type in [\"mat\", \"pmatrices\"]:\n",
    "            data_array = load_annotations_from_mat(ann_file, ann_type)\n",
    "        elif ann_type in [\"labels\", \"ground_truth\"]:\n",
    "            data_array = load_annotations_from_text(ann_file)\n",
    "\n",
    "        if data_array is not None:\n",
    "            print(f\"  Annotation array shape: {data_array.shape}\")\n",
    "            gt_dict = process_annotation_array(data_array, ann_type, folder_path)\n",
    "        else:\n",
    "            print(\"  -> Failed to load annotation data.\")\n",
    "    else:\n",
    "        print(\"  -> No annotation file found.\")\n",
    "\n",
    "    # Process images\n",
    "    pos_count = neg_count = 0\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "\n",
    "        # Extract image ID using regex to find last numeric sequence\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+', base_name)\n",
    "            if not numbers:\n",
    "                raise ValueError(f\"No numbers in filename: {file}\")\n",
    "            image_id = int(numbers[-1])  # Use last number as ID\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error extracting ID from {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Determine label\n",
    "        if is_labels_folder:\n",
    "            # If it's a labels folder and not excluded, mark as positive\n",
    "            label = 1\n",
    "        elif ann_file is not None:\n",
    "            # If an annotation file exists, use it to determine the label\n",
    "            label = 1 if (gt_dict and image_id in gt_dict) else 0\n",
    "        elif folder not in EXCLUDED_FOLDERS:\n",
    "            # No annotation file found, and folder is not excluded: default to positive\n",
    "            label = 1\n",
    "        else:\n",
    "            # Folder is excluded and no annotation file found: mark as negative\n",
    "            label = 0\n",
    "\n",
    "        # Update counters\n",
    "        if label == 1:\n",
    "            pos_count += 1\n",
    "        else:\n",
    "            neg_count += 1\n",
    "\n",
    "        data_entries.append({\n",
    "            'subfolder': folder,\n",
    "            'image_path': image_path,\n",
    "            'image_id': image_id,\n",
    "            'label': label,\n",
    "            'bboxes': gt_dict.get(image_id, []) if gt_dict else []\n",
    "        })\n",
    "\n",
    "    print(f\"  -> Labeling results: Pos={pos_count}, Neg={neg_count}, Total={pos_count + neg_count}\")\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(data_entries)\n",
    "df.to_csv('baggage_labels.csv', index=False)\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "print(f\"Total images: {len(df)} (Positive: {sum(df['label']==1)}, Negative: {sum(df['label']==0)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8653 (Class 0: 1940, Class 1: 6713)\n",
      "Validation samples: 1082\n",
      "Test samples: 1082\n",
      "Data loaders successfully created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "class AdvancedXRayDataset(Dataset):\n",
    "    \"\"\"Enhanced X-ray dataset with robust error handling and advanced augmentations\"\"\"\n",
    "    def __init__(self, file_paths, labels, is_training=False):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # Base preprocessing pipeline\n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize((384, 384)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Advanced augmentation pipeline\n",
    "        self.aug_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.3),\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)), \n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(kernel_size=(3, 7))], p=0.3),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "            transforms.RandomAutocontrast(),\n",
    "            transforms.RandomPosterize(bits=3, p=0.2)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for _ in range(3):  # Allow 3 attempts to load image\n",
    "            try:\n",
    "                img_path = self.file_paths[idx]\n",
    "                label = self.labels[idx]\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                if self.is_training:\n",
    "                    image = self.aug_transform(image)\n",
    "                \n",
    "                return self.base_transform(image), torch.tensor(label, dtype=torch.float32)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                idx = np.random.randint(0, len(self))  # Fallback to random sample\n",
    "        return torch.zeros(3, 384, 384), torch.tensor(0.0)  # Ultimate fallback\n",
    "\n",
    "def create_data_loaders(csv_path, test_size=0.1, val_size=0.1, batch_size=64):\n",
    "    \"\"\"Create stratified train/val/test loaders with class balancing\"\"\"\n",
    "    # Load and filter data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['image_path'].apply(os.path.exists)].sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Initial split (train+val vs test)\n",
    "    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "        df['image_path'].values, df['label'].values.astype('float32'),\n",
    "        test_size=test_size, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Secondary split (train vs val)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_val_paths, train_val_labels,\n",
    "        test_size=val_size/(1-test_size), \n",
    "        stratify=train_val_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Class balancing setup\n",
    "    class_counts = np.bincount(train_labels.astype(int))\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = class_weights[train_labels.astype(int)]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = AdvancedXRayDataset(train_paths, train_labels, is_training=True)\n",
    "    val_dataset = AdvancedXRayDataset(val_paths, val_labels)\n",
    "    test_dataset = AdvancedXRayDataset(test_paths, test_labels)\n",
    "\n",
    "    # Create loaders\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size, sampler=sampler,\n",
    "                           num_workers=os.cpu_count(), pin_memory=True),\n",
    "        'val': DataLoader(val_dataset, batch_size, shuffle=False,\n",
    "                         num_workers=os.cpu_count(), pin_memory=True),\n",
    "        'test': DataLoader(test_dataset, batch_size, shuffle=False,\n",
    "                          num_workers=os.cpu_count(), pin_memory=True)\n",
    "    }\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(f\"Training samples: {len(train_dataset)} (Class 0: {class_counts[0]}, Class 1: {class_counts[1]})\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return loaders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DATA_CSV_PATH = 'baggage_labels.csv'\n",
    "    loaders = create_data_loaders(DATA_CSV_PATH)\n",
    "    print(\"Data loaders successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Stage 1: {'img_size': 224, 'batch_size': 64, 'epochs': 10, 'lr': 0.0001} ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\AppData\\Local\\Temp\\ipykernel_5964\\3317398376.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\dania\\AppData\\Local\\Temp\\ipykernel_5964\\3317398376.py:28: UserWarning: 1 validation error for InitSchema\n",
      "size\n",
      "  Input should be a valid tuple [type=tuple_type, input_value=224, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type\n",
      "  A.RandomResizedCrop(img_size, img_size, scale=(0.67, 1.0)),\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "6 validation errors for InitSchema\np\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nscale\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nratio\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsize\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ninterpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nmask_interpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 274\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 274\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 155\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    148\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m CosineLRScheduler(optimizer, \n\u001b[0;32m    149\u001b[0m                             t_initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m prog_trainer\u001b[38;5;241m.\u001b[39mstages),\n\u001b[0;32m    150\u001b[0m                             warmup_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    151\u001b[0m                             lr_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m,\n\u001b[0;32m    152\u001b[0m                             warmup_lr_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Update transforms and data loaders\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m train_transform, val_transform \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m WeaponDataset(train_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train_transform)\n\u001b[0;32m    157\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m WeaponDataset(val_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, val_transform)\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mcreate_transforms\u001b[1;34m(img_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_transforms\u001b[39m(img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m):\n\u001b[0;32m     27\u001b[0m     train_transform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomResizedCrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.67\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     29\u001b[0m         A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     30\u001b[0m         A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     31\u001b[0m         A\u001b[38;5;241m.\u001b[39mShiftScaleRotate(shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, scale_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, rotate_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     32\u001b[0m         A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(brightness_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, contrast_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     33\u001b[0m         A\u001b[38;5;241m.\u001b[39mCoarseDropout(max_holes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, max_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     34\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m     35\u001b[0m         ToTensorV2()\n\u001b[0;32m     36\u001b[0m     ])\n\u001b[0;32m     38\u001b[0m     val_transform \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     39\u001b[0m         A\u001b[38;5;241m.\u001b[39mResize(img_size, img_size),\n\u001b[0;32m     40\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m     41\u001b[0m         ToTensorV2()\n\u001b[0;32m     42\u001b[0m     ])\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_transform, val_transform\n",
      "File \u001b[1;32mc:\\Users\\dania\\testPytorch\\env1\\lib\\site-packages\\albumentations\\core\\validation.py:48\u001b[0m, in \u001b[0;36mValidatedTransformMeta.__new__.<locals>.custom_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m warn(\u001b[38;5;28mstr\u001b[39m(e), stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Use default values for invalid parameters\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mdct\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m validated_kwargs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[0;32m     50\u001b[0m validated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Also remove from default values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dania\\testPytorch\\env1\\lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 6 validation errors for InitSchema\np\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nscale\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nratio\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsize\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ninterpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nmask_interpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from timm.optim import AdamP\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1Score, AUROC, AveragePrecision\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# -------------------- Hardware Optimization --------------------\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# -------------------- Data Augmentations --------------------\n",
    "def create_transforms(img_size=224):\n",
    "    train_transform = A.Compose([\n",
    "        A.RandomResizedCrop(img_size, img_size, scale=(0.67, 1.0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class WeaponDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.dataframe.loc[idx, 'image_path'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.dataframe.loc[idx, 'label']\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "            \n",
    "        return image, torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "# -------------------- Model Architecture --------------------\n",
    "class WeaponDetector(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.base_model = timm.create_model('tf_efficientnetv2_l', \n",
    "                                           pretrained=True,\n",
    "                                           num_classes=0,\n",
    "                                           features_only=False)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1280, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        return self.head(features)\n",
    "\n",
    "# -------------------- Training Utilities --------------------\n",
    "class ProgressiveTrainer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.stages = [\n",
    "            {'img_size': 224, 'batch_size': 64, 'epochs': 10, 'lr': 1e-4},\n",
    "            {'img_size': 384, 'batch_size': 32, 'epochs': 15, 'lr': 5e-5},\n",
    "            {'img_size': 512, 'batch_size': 16, 'epochs': 25, 'lr': 1e-5}\n",
    "        ]\n",
    "        self.current_stage = 0\n",
    "        self.global_epoch = 0\n",
    "        \n",
    "    def get_current_config(self):\n",
    "        return self.stages[self.current_stage]\n",
    "    \n",
    "    def progress_stage(self):\n",
    "        if self.current_stage < len(self.stages) - 1:\n",
    "            self.current_stage += 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "def train_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Data preparation\n",
    "    full_df = pd.read_csv('baggage_labels.csv')\n",
    "    train_df, test_df = train_test_split(full_df, test_size=0.15, stratify=full_df['label'], random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)\n",
    "\n",
    "    # Model setup\n",
    "    model = WeaponDetector().to(device)\n",
    "    ema_model = AveragedModel(model).to(device)\n",
    "    scaler = GradScaler()\n",
    "    prog_trainer = ProgressiveTrainer(model, device)\n",
    "    \n",
    "    # Loss function\n",
    "    pos_weight = torch.tensor([len(train_df) / (2 * train_df['label'].sum())]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = MetricCollection({\n",
    "        'accuracy': Accuracy(task='binary'),\n",
    "        'precision': Precision(task='binary'),\n",
    "        'recall': Recall(task='binary'),\n",
    "        'f1': F1Score(task='binary'),\n",
    "        'auc': AUROC(task='binary'),\n",
    "        'prc': AveragePrecision(task='binary')\n",
    "    }).to(device)\n",
    "\n",
    "    best_score = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    while True:\n",
    "        config = prog_trainer.get_current_config()\n",
    "        print(f\"\\n=== Stage {prog_trainer.current_stage + 1}: {config} ===\")\n",
    "        \n",
    "        # Reinitialize optimizer/scheduler for each stage\n",
    "        optimizer = AdamP(model.parameters(), lr=config['lr'], weight_decay=1e-4)\n",
    "        scheduler = CosineLRScheduler(optimizer, \n",
    "                                    t_initial=sum(s['epochs'] for s in prog_trainer.stages),\n",
    "                                    warmup_t=10,\n",
    "                                    lr_min=1e-6,\n",
    "                                    warmup_lr_init=5e-5)\n",
    "        \n",
    "        # Update transforms and data loaders\n",
    "        train_transform, val_transform = create_transforms(config['img_size'])\n",
    "        train_dataset = WeaponDataset(train_df, './data', train_transform)\n",
    "        val_dataset = WeaponDataset(val_df, './data', val_transform)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=config['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                num_workers=12,\n",
    "                                pin_memory=True,\n",
    "                                persistent_workers=True,\n",
    "                                drop_last=True)\n",
    "        \n",
    "        val_loader = DataLoader(val_dataset,\n",
    "                              batch_size=config['batch_size'],\n",
    "                              num_workers=8,\n",
    "                              pin_memory=True)\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            model.train()\n",
    "            prog_bar = tqdm(train_loader, desc=f\"Epoch {prog_trainer.global_epoch+1}/{sum(s['epochs'] for s in prog_trainer.stages)}\")\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for images, labels in prog_bar:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                ema_model.update_parameters(model)\n",
    "                \n",
    "                epoch_loss += loss.item() * images.size(0)\n",
    "                prog_bar.set_postfix({'loss': loss.item(), 'lr': optimizer.param_groups[0]['lr']})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_metrics = {k: [] for k in metrics.keys()}\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    with autocast():\n",
    "                        outputs = ema_model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        preds = torch.sigmoid(outputs)\n",
    "                    \n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    batch_metrics = metrics(preds, labels)\n",
    "                    for k in val_metrics:\n",
    "                        val_metrics[k].append(batch_metrics[k].item())\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            avg_val_metrics = {k: np.mean(v) for k, v in val_metrics.items()}\n",
    "            combined_score = avg_val_metrics['auc'] + avg_val_metrics['prc']\n",
    "            \n",
    "            print(f\"Epoch {prog_trainer.global_epoch+1} | \"\n",
    "                f\"Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                f\"AUC: {avg_val_metrics['auc']:.4f} | PRC: {avg_val_metrics['prc']:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if combined_score > best_score:\n",
    "                best_score = combined_score\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'ema_model': ema_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'global_epoch': prog_trainer.global_epoch,\n",
    "                    'metrics': avg_val_metrics\n",
    "                }, 'best_model.pth')\n",
    "                print(f\"New best model saved with combined score: {best_score:.4f}\")\n",
    "\n",
    "            # Update scheduler and epoch counter\n",
    "            scheduler.step(prog_trainer.global_epoch)\n",
    "            prog_trainer.global_epoch += 1\n",
    "\n",
    "        # Update batch norm statistics for EMA model\n",
    "        update_bn(train_loader, ema_model, device=device)\n",
    "        \n",
    "        # Progress to next stage or exit\n",
    "        if not prog_trainer.progress_stage():\n",
    "            break\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    test_transform = create_transforms(prog_trainer.stages[-1]['img_size'])[1]\n",
    "    test_dataset = WeaponDataset(test_df, './data', test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=prog_trainer.stages[-1]['batch_size'], num_workers=8)\n",
    "    \n",
    "    ema_model.eval()\n",
    "    test_metrics = {k: [] for k in metrics.keys()}\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = ema_model(images)\n",
    "                preds = torch.sigmoid(outputs)\n",
    "            \n",
    "            batch_metrics = metrics(preds, labels)\n",
    "            for k in test_metrics:\n",
    "                test_metrics[k].append(batch_metrics[k].item())\n",
    "    \n",
    "    avg_test_metrics = {k: np.mean(v) for k, v in test_metrics.items()}\n",
    "    print(\"\\n=== Final Test Metrics ===\")\n",
    "    for k, v in avg_test_metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
